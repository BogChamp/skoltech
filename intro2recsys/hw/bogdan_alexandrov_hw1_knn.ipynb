{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, diags, isspmatrix\n",
    "\n",
    "\n",
    "from polara import get_movielens_data\n",
    "from polara.preprocessing.dataframes import leave_one_out, reindex\n",
    "\n",
    "from dataprep import transform_indices, verify_time_split, generate_interactions_matrix, \\\n",
    "                    cosine_similarity_zd, leave_last_out\n",
    "from evaluation import topn_recommendations, model_evaluate, downvote_seen_items\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement two variants of user-based KNN for the top-$n$ recommendations task when:\n",
    "1. similarity matrix is symmetric,\n",
    "2. similarity matrix is asymmetric.\n",
    "\n",
    "Recall, there's no reason for implementing row-wise weighting scheme in user-based KNN. So choose the weighting scheme wisely.\n",
    "\n",
    " In your experiments:  \n",
    "- Test your solution against both weak and strong generalization. \n",
    "  - In total you'll have 4 different experiments.\n",
    "- Follow the \"most-recent-item\" sampling strategy for constructing holdout.\n",
    "  - Explain potential issues of this scheme in relation to both weak and strong generalization.  \n",
    "- Report evaluation metrics, compare the models, and analyse the results.  \n",
    "- Use Movielens-1M data.\n",
    "\n",
    "**Note**: you can reuse some code from seminars if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_movielens_data(include_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak generalization test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is\n",
    "- split data into training and holdout parts\n",
    "- build a new internal contiguous representation of user and item index based on the training data\n",
    "- make sure same index is used in the holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split most recent holdout item from each user\n",
    "training_, holdout_ = leave_one_out(\n",
    "    data,\n",
    "    target='timestamp',\n",
    "    sample_top=True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# check correct time splitting\n",
    "verify_time_split(training_, holdout_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 2 invalid observations.\n"
     ]
    }
   ],
   "source": [
    "# reindex data to make contiguous index starting from 0 for user and item IDs\n",
    "training, data_index = transform_indices(training_, 'userid', 'movieid')\n",
    "\n",
    "# apply new index to the holdout data\n",
    "holdout = reindex(holdout_, data_index.values(), filter_invalid=True)\n",
    "holdout = holdout.sort_values('userid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's also populate data description dictionary for convenience.\n",
    "- It allows using uniform names for users and items field.\n",
    "  - This way the code does't depend on the actual names in you dataset.\n",
    "  - So later you can easily switch to another dataset without changing the code fo the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description = dict(\n",
    "    users = data_index['users'].name,\n",
    "    items = data_index['items'].name,\n",
    "    feedback = 'rating',\n",
    "    n_users = len(data_index['users']),\n",
    "    n_items = len(data_index['items']),\n",
    "    test_users = holdout[data_index['users'].name].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously, let's also explicitly store our testset (i.e., ratings of test users excluding holdout items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid = data_description['users']\n",
    "seen_idx_mask = training[userid].isin(data_description['test_users'])\n",
    "testset = training[seen_idx_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric case (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can consult the code from seminars or implement your own solution as long as it is fast enough.\n",
    "\n",
    "- Recall that subsampling of the neighborhood not only makes the algorithm run faster, but can also improve the results.  \n",
    "- **Make sure to implement some kind of neighborhood subsampling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_similarity(similarity, k=30):\n",
    "    '''\n",
    "    For every row in similarity matrix, pick at most k entities\n",
    "    with the highest similarity scores. Disregard everything else.\n",
    "    '''\n",
    "    similarity = similarity.tocsr()\n",
    "    inds = similarity.indices\n",
    "    ptrs = similarity.indptr\n",
    "    data = similarity.data\n",
    "    new_ptrs = [0]\n",
    "    new_inds = []\n",
    "    new_data = []\n",
    "    for i in range(len(ptrs)-1):\n",
    "        start, stop = ptrs[i], ptrs[i+1]\n",
    "        if start < stop:\n",
    "            data_ = data[start:stop]\n",
    "            topk = min(len(data_), k)\n",
    "            idx = np.argpartition(data_, -topk)[-topk:]\n",
    "            new_data.append(data_[idx])\n",
    "            new_inds.append(inds[idx+start])\n",
    "            new_ptrs.append(new_ptrs[-1]+len(idx))\n",
    "        else:\n",
    "            new_ptrs.append(new_ptrs[-1])\n",
    "    new_data = np.concatenate(new_data)\n",
    "    new_inds = np.concatenate(new_inds)\n",
    "    truncated = csr_matrix(\n",
    "        (new_data, new_inds, new_ptrs),\n",
    "        shape=similarity.shape\n",
    "    )\n",
    "    return truncated    \n",
    "\n",
    "\n",
    "def build_uknn_model(config, data, data_description):\n",
    "    user_item_mtx = generate_interactions_matrix(data, data_description)\n",
    "\n",
    "    # compute similarity matrix\n",
    "    user_similarity = truncate_similarity(\n",
    "        cosine_similarity_zd(user_item_mtx),\n",
    "        config['n_neighbors']\n",
    "    )\n",
    "    weighted = config['weighted']\n",
    "    return user_item_mtx, user_similarity, weighted\n",
    "\n",
    "\n",
    "def uknn_model_scoring(params, testset, testset_description):\n",
    "    # implement the scoring function to assign scores\n",
    "    # to all items for test users\n",
    "    user_item_mtx, user_similarity, weighted = params\n",
    "    # write your code for scoring, don't forget to return a dense array\n",
    "\n",
    "    test_users = testset_description['test_users']\n",
    "    scores = user_similarity.dot(user_item_mtx)\n",
    "\n",
    "    if weighted == 'unweighted':\n",
    "        return scores.toarray()[test_users, :]\n",
    "    elif weighted == 'elementwise':\n",
    "        normalizer = user_similarity.dot(user_item_mtx.astype('bool'))\n",
    "        scores = np.nan_to_num(np.divide(scores, normalizer))\n",
    "        return np.array(scores[test_users, :])\n",
    "    else:\n",
    "        raise Exception('wrong weighting scheme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 100\n",
    "\n",
    "uknn_params_uw = build_uknn_model(\n",
    "    {'weighted': 'unweighted', 'n_neighbors': n_neighbors}, training, data_description\n",
    ")\n",
    "uknn_params_ew = build_uknn_model(\n",
    "    {'weighted': 'elementwise', 'n_neighbors': n_neighbors}, training, data_description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_scores_uw = uknn_model_scoring(uknn_params_uw, None, data_description)\n",
    "uknn_scores_ew = uknn_model_scoring(uknn_params_ew, None, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "downvote_seen_items(uknn_scores_uw, testset, data_description)\n",
    "downvote_seen_items(uknn_scores_ew, testset, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs_uw = topn_recommendations(uknn_scores_uw)\n",
    "uknn_recs_ew = topn_recommendations(uknn_scores_ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighting mode: unweighted\n",
      "HR=0.085, MRR=0.0286, COV=0.176\n",
      "\n",
      "Weighting mode: elementwise\n",
      "HR=0.000994, MRR=0.000426, COV=0.755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modes = ['unweighted', 'elementwise']\n",
    "uknn_recs = dict(zip(modes, [uknn_recs_uw, uknn_recs_ew]))\n",
    "\n",
    "\n",
    "uknn_metrics = {}\n",
    "for mode, recs in uknn_recs.items():\n",
    "    uknn_metrics[mode] = metrics = model_evaluate(recs, holdout, data_description)\n",
    "    print(\n",
    "        f'Weighting mode: {mode}\\n'\\\n",
    "        'HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: recommending items from user history doesn't make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymmetric case (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your task here is to implement user-based KNN with asymmetric similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uknn_model_asym(config, data, data_description, alpha=1):\n",
    "    user_item_mtx = generate_interactions_matrix(data, data_description)\n",
    "    user_similarity = truncate_similarity(\n",
    "        cosine_similarity_zd(user_item_mtx),\n",
    "        config['n_neighbors']\n",
    "    )\n",
    "    \n",
    "    D = np.array(user_similarity.sum(axis=0)).squeeze()\n",
    "    normalizer = diags(np.divide(\n",
    "            1,\n",
    "            D,\n",
    "            where=(D!=0)\n",
    "        )).power(alpha)\n",
    "    user_similarity = user_similarity.dot(normalizer)\n",
    "    \n",
    "    return user_item_mtx, user_similarity\n",
    "\n",
    "\n",
    "def uknn_model_scoring_asym(params, testset, testset_description):\n",
    "    user_item_mtx, user_similarity = params\n",
    "    test_users = testset_description['test_users']\n",
    "    \n",
    "    scores = user_similarity.dot(user_item_mtx)\n",
    "    return scores[test_users, :].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_params_asym = build_uknn_model_asym(\n",
    "    {'weighting': False, 'n_neighbors': n_neighbors}, training, data_description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_scores_asym = uknn_model_scoring_asym(uknn_params_asym, None, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "downvote_seen_items(uknn_scores_asym, testset, data_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluation (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate top-$n$ recommendations for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs = topn_recommendations(uknn_scores_uw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs_asym = topn_recommendations(uknn_scores_asym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity type: symmetric\n",
      "HR=0.085, MRR=0.0286, COV=0.176\n",
      "\n",
      "Similarity type: asymmetric\n",
      "HR=0.0886, MRR=0.0307, COV=0.275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modes = ['symmetric', 'asymmetric']\n",
    "uknn_recs = dict(zip(modes, [uknn_recs, uknn_recs_asym]))\n",
    "\n",
    "\n",
    "uknn_metrics = {}\n",
    "for mode, recs in uknn_recs.items():\n",
    "    if recs is None: continue\n",
    "    uknn_metrics[mode] = metrics = model_evaluate(recs, holdout, data_description)\n",
    "    print(\n",
    "        f'Similarity type: {mode}\\n'\\\n",
    "        'HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strong generalization test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that in the strong generalization test you work with the warm-start scenario.\n",
    "- It means that the set of test users is disjoint from the set of users in the training.\n",
    "- You're provided with the basic functions to help you perform correct splitting, but there're still a few places where your input is required. Make sure you understand the logic of data splitting in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your task is to select a subset of users who have the most recent interactions in their history across entire dataset.\n",
    "- You will apply holdout splitting to only this subset.\n",
    "  - Think, why simply taking all users (as in weak generalization test) makes no sense in this scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_time(data, time_q=0.95, timeid='timestamp'):\n",
    "    '''\n",
    "    Split the input `data` DataFrame into two parts based on the timestamp, with the split point\n",
    "    being determined by the quantile value `time_q`. The function returns a tuple `(before, after)`\n",
    "    containing the two DataFrames. The `after` DataFrame contains the rows with timestamps greater\n",
    "    than or equal to the split point, while the `before` DataFrame contains the remaining rows. \n",
    "\n",
    "    Details:\n",
    "    The `quantile` method of the pandas DataFrame is used to calculate the time point (i.e., timestamp)\n",
    "    that divides the data into two parts based on the given quantile value `time_q`. Specifically,\n",
    "    the time point `split_timepoint` is calculated as the `time_q`th quantile of the values in the `timeid`\n",
    "    column of the `data` DataFrame, using the interpolation method of `nearest`. This means that\n",
    "    `split_timepoint` is the timestamp at or immediately after which `time_q` percent of the data points occur.    \n",
    "    '''\n",
    "    split_timepoint = data[timeid].quantile(q=time_q, interpolation='nearest')\n",
    "    after = data.query(f'{timeid} >= @split_timepoint') \n",
    "    before = data.drop(after.index)\n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, you need to select a candidate subset of observations, from which you'll construct the the training, testset, and holdout datssets. Check the `split_by_time` function below and its description in the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "before, after = split_by_time(data, time_q=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now it's time to perform holdout sampling based on the obtained timepoint splitting. \n",
    "- Remember, you only sample from the test users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_part_, holdout_ = leave_last_out(after)# your code for holdout sampling\n",
    "\n",
    "# verify correctness of time-based splitting,\n",
    "# i.e., for each test user, the holdout contains only future interactions w.r.t to testset\n",
    "test_indices = testset_part_['userid'].values\n",
    "holdout_ = holdout_[holdout_.userid.isin(test_indices)]\n",
    "verify_time_split(testset_part_, holdout_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ = before[~before.userid.isin(test_indices)] # recall that training and testset must be disjoint by users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that `testset_part_` only contains interactions of the test users **after the timepoint**.\n",
    "- You need to combine it with the remaining histories of these users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all test users data into a single `testset_` Dataframe.\n",
    "testset_ = pd.concat(\n",
    "    [before[before.userid.isin(test_indices)], testset_part_],\n",
    "    axis = 0,\n",
    "    ignore_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building internal representation of user and item index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `transform_indices` function for building a contiguous index starting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_strong, data_index_strong = transform_indices(training_, 'userid', 'movieid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before applying new index to the test data:\n",
    "  - note that the users in the `testset` must be the same as the users in the `holdout`.\n",
    "- Below is the corresponding function `align_test_by_users` that ensures these two datasets' alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_test_by_users(testset, holdout):\n",
    "    test_users = np.intersect1d(holdout['userid'].values, testset['userid'].values)\n",
    "    # only allow the same users to be present in both datasets\n",
    "    testset = testset.query('userid in @test_users').sort_values('userid')\n",
    "    holdout = holdout.query('userid in @test_users').sort_values('userid')\n",
    "    return testset, holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply new item index to test data and finalize the test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 4 invalid observations.\n",
      "Filtered 109 invalid observations.\n"
     ]
    }
   ],
   "source": [
    "holdout_strong = reindex(holdout_, data_index_strong['items'], filter_invalid=True)\n",
    "testset_strong = reindex(testset_, data_index_strong['items'], filter_invalid=True)\n",
    "\n",
    "testset_strong, holdout_strong = align_test_by_users(testset_strong, holdout_strong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Think why we do not apply new index to users here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section you'll need to implement user-based KNN models for the warm-start scenario.\n",
    "- Think carefully which data must be generated at the build time and which data must be generated in the scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric case (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uknn_model_strong(config, data, data_description):\n",
    "    user_item_mtx = generate_interactions_matrix(data, data_description)\n",
    "    weighted = config['weighted']\n",
    "    n_size = config['n_neighbors']\n",
    "    return user_item_mtx, weighted, n_size\n",
    "\n",
    "\n",
    "def uknn_model_scoring_strong(params, testset, testset_description):\n",
    "    user_item_mtx, weighted, n_size = params\n",
    "    user_item_mtx_test = generate_interactions_matrix(testset, testset_description, rebase_users=True)\n",
    "\n",
    "    similarity = cosine_similarity(user_item_mtx_test, user_item_mtx, dense_output=False)\n",
    "    similarity.eliminate_zeros()\n",
    "    similarity = similarity.tocsr()\n",
    "    \n",
    "    user_similarity = truncate_similarity(\n",
    "        similarity,\n",
    "        n_size\n",
    "    )\n",
    "    \n",
    "    scores = user_similarity.dot(user_item_mtx)\n",
    "    \n",
    "    if not weighted:\n",
    "        return scores.toarray()\n",
    "    \n",
    "    normalizer = user_similarity.dot(user_item_mtx.astype('bool'))\n",
    "    scores = np.nan_to_num(np.divide(scores, normalizer))\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 30\n",
    "\n",
    "data_description_strong = dict(\n",
    "    users = data_index_strong['users'].name,\n",
    "    items = data_index_strong['items'].name,\n",
    "    feedback = 'rating',\n",
    "    n_users = len(data_index_strong['users']),\n",
    "    n_items = len(data_index_strong['items']),\n",
    "    test_users = holdout_strong[data_index_strong['users'].name].values\n",
    ")\n",
    "\n",
    "uknn_params_uw = build_uknn_model_strong(\n",
    "    {'weighted': False, 'n_neighbors': n_neighbors}, training_strong, data_description_strong\n",
    ")\n",
    "\n",
    "uknn_params_ew = build_uknn_model_strong(\n",
    "    {'weighted': True, 'n_neighbors': n_neighbors}, training_strong, data_description_strong\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_description_strong = dict(\n",
    "    users = data_index_strong['users'].name,\n",
    "    items = data_index_strong['items'].name,\n",
    "    feedback = 'rating',\n",
    "    n_users = len(holdout_strong),\n",
    "    n_items = len(data_index_strong['items']),\n",
    "    test_users = holdout_strong[data_index_strong['users'].name].values\n",
    ")\n",
    "\n",
    "uknn_scores_uw = uknn_model_scoring_strong(uknn_params_uw, testset_strong, test_description_strong)\n",
    "uknn_scores_ew = uknn_model_scoring_strong(uknn_params_ew, testset_strong, test_description_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "downvote_seen_items(uknn_scores_uw, testset_strong, test_description_strong)\n",
    "downvote_seen_items(uknn_scores_ew, testset_strong, test_description_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs_uw = topn_recommendations(uknn_scores_uw)\n",
    "uknn_recs_ew = topn_recommendations(uknn_scores_ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighting mode: unweighted\n",
      "HR=0.0581, MRR=0.0205, COV=0.151\n",
      "\n",
      "Weighting mode: elementwise\n",
      "HR=0.00387, MRR=0.00237, COV=0.449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modes = ['unweighted', 'elementwise']\n",
    "uknn_recs = dict(zip(modes, [uknn_recs_uw, uknn_recs_ew]))\n",
    "\n",
    "\n",
    "uknn_metrics = {}\n",
    "for mode, recs in uknn_recs.items():\n",
    "    uknn_metrics[mode] = metrics = model_evaluate(recs, holdout_strong, data_description_strong)\n",
    "    print(\n",
    "        f'Weighting mode: {mode}\\n'\\\n",
    "        'HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymmetric case (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uknn_model_asym_strong(config, data, data_description):\n",
    "    user_item_mtx = generate_interactions_matrix(data, data_description)\n",
    "    weighted = config['weighted']\n",
    "    n_size = config['n_neighbors']\n",
    "    return user_item_mtx, weighted, n_size\n",
    "\n",
    "def uknn_model_scoring_asym_strong(params, testset, testset_description, alpha=1):\n",
    "    user_item_mtx, weighted, n_size = params\n",
    "    user_item_mtx_test = generate_interactions_matrix(testset, testset_description, rebase_users=True)\n",
    "\n",
    "    similarity = cosine_similarity(user_item_mtx_test, user_item_mtx, dense_output=False)\n",
    "    similarity.eliminate_zeros()\n",
    "    similarity = similarity.tocsr()\n",
    "    \n",
    "    user_similarity = truncate_similarity(\n",
    "        similarity,\n",
    "        n_size\n",
    "    )\n",
    "    \n",
    "    D = np.array(user_similarity.sum(axis=0)).squeeze()\n",
    "    normalizer = diags(np.divide(\n",
    "            1,\n",
    "            D,\n",
    "            where=(D!=0)\n",
    "        )).power(alpha)\n",
    "    user_similarity = user_similarity.dot(normalizer)\n",
    "    scores = user_similarity.dot(user_item_mtx)\n",
    "    return scores.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_params_asym = build_uknn_model_strong(\n",
    "    {'weighted': False, 'n_neighbors': n_neighbors}, training_strong, data_description_strong\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_scores_asym = uknn_model_scoring_asym_strong(uknn_params_asym, testset_strong, test_description_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "downvote_seen_items(uknn_scores_asym, testset_strong, test_description_strong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluation (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate recommendations for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs = topn_recommendations(uknn_scores_uw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs_asym = topn_recommendations(uknn_scores_asym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity type: symmetric\n",
      "HR=0.0581, MRR=0.0205, COV=0.151\n",
      "\n",
      "Similarity type: asymmetric\n",
      "HR=0.049, MRR=0.018, COV=0.228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modes = ['symmetric', 'asymmetric']\n",
    "uknn_recs = dict(zip(modes, [uknn_recs, uknn_recs_asym]))\n",
    "\n",
    "\n",
    "uknn_metrics = {}\n",
    "for mode, recs in uknn_recs.items():\n",
    "    if recs is None: continue\n",
    "    uknn_metrics[mode] = metrics = model_evaluate(recs, holdout_strong, test_description_strong)\n",
    "    print(\n",
    "        f'Similarity type: {mode}\\n'\\\n",
    "        'HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning (2 pts)\n",
    "- Try to find a neighborhood size that gives you better results.\n",
    "- Perform a simple grid-search experiment and report your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(build_model, weighting, n_size, train, test, holdout, desc_train, desc_test, scores):\n",
    "    uknn_params = build_model(\n",
    "    {'weighted': weighting, 'n_neighbors': n_size}, train, desc_train\n",
    "    )\n",
    "    uknn_scores = scores(uknn_params, test, desc_test)\n",
    "    downvote_seen_items(uknn_scores, test, desc_test)\n",
    "    uknn_recs = topn_recommendations(uknn_scores)\n",
    "    metrics = model_evaluate(uknn_recs, holdout, desc_train)\n",
    "    return metrics\n",
    "\n",
    "def update_metrics(metrics, best_m, best_n, n_size):\n",
    "    if metrics[0] > best_m[0]:\n",
    "        best_m[0] = metrics[0]\n",
    "        best_n[0] = n_size\n",
    "    if metrics[1] > best_m[1]:\n",
    "        best_m[1] = metrics[1]\n",
    "        best_n[1] = n_size\n",
    "    if metrics[2] > best_m[2]:\n",
    "        best_m[2] = metrics[2]\n",
    "        best_n[2] = n_size\n",
    "\n",
    "\n",
    "best_weak = [0, 0, 0]\n",
    "best_weak_n = [0, 0, 0]\n",
    "best_weak_asym = [0, 0, 0]\n",
    "best_weak_asym_n = [0, 0, 0]\n",
    "\n",
    "best_strong = [0, 0, 0]\n",
    "best_strong_n = [0, 0, 0]\n",
    "best_strong_asym = [0, 0, 0]\n",
    "best_strong_asym_n = [0, 0, 0]\n",
    "\n",
    "for n_neighbors in [2, 3, 4, 5, 10, 20, 50, 100, 200]:\n",
    "    metrics_weak = calculate_metrics(build_uknn_model, 'unweighted', n_neighbors,\n",
    "                                    training, testset, holdout, data_description, \n",
    "                                    data_description, uknn_model_scoring)\n",
    "    update_metrics(metrics_weak, best_weak, best_weak_n, n_neighbors)\n",
    "    \n",
    "    metrics_weak_asym = calculate_metrics(build_uknn_model_asym, None, n_neighbors,\n",
    "                                    training, testset, holdout, data_description, \n",
    "                                    data_description, uknn_model_scoring_asym)\n",
    "    update_metrics(metrics_weak_asym, best_weak_asym, best_weak_asym_n, n_neighbors)\n",
    "    \n",
    "    metrics_strong = calculate_metrics(build_uknn_model_strong, False, n_neighbors,\n",
    "                                    training_strong, testset_strong, holdout_strong, data_description_strong, \n",
    "                                    test_description_strong, uknn_model_scoring_strong)\n",
    "    update_metrics(metrics_strong, best_strong, best_strong_n, n_neighbors)\n",
    "    \n",
    "    metrics_strong_asym = calculate_metrics(build_uknn_model_asym_strong, None, n_neighbors,\n",
    "                                    training_strong, testset_strong, holdout_strong, data_description_strong, \n",
    "                                    test_description_strong, uknn_model_scoring_asym_strong)\n",
    "    update_metrics(metrics_strong_asym, best_strong_asym, best_strong_asym_n, n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best neigborhood sizes for weak-generalization are:\n",
      "best HR score:0.087, neighborhood size:20\n",
      "best MRR score:0.030, neighborhood size:20\n",
      "best COV score:0.589, neighborhood size:2\n",
      "\n",
      "Best neigborhood sizes for weak-generalization asymmetric are:\n",
      "best HR score:0.090, neighborhood size:200\n",
      "best MRR score:0.031, neighborhood size:200\n",
      "best COV score:0.643, neighborhood size:2\n",
      "\n",
      "Best neigborhood sizes for strong-generalization are:\n",
      "best HR score:0.062, neighborhood size:100\n",
      "best MRR score:0.023, neighborhood size:200\n",
      "best COV score:0.313, neighborhood size:2\n",
      "\n",
      "Best neigborhood sizes for strong-generalization asymmetric are:\n",
      "best HR score:0.059, neighborhood size:20\n",
      "best MRR score:0.022, neighborhood size:200\n",
      "best COV score:0.329, neighborhood size:2\n"
     ]
    }
   ],
   "source": [
    "print('Best neigborhood sizes for weak-generalization are:')\n",
    "print('best HR score:{:.3f}, neighborhood size:{}'.format(best_weak[0], best_weak_n[0]))\n",
    "print('best MRR score:{:.3f}, neighborhood size:{}'.format(best_weak[1], best_weak_n[1]))\n",
    "print('best COV score:{:.3f}, neighborhood size:{}'.format(best_weak[2], best_weak_n[2]))\n",
    "print()\n",
    "print('Best neigborhood sizes for weak-generalization asymmetric are:')\n",
    "print('best HR score:{:.3f}, neighborhood size:{}'.format(best_weak_asym[0], best_weak_asym_n[0]))\n",
    "print('best MRR score:{:.3f}, neighborhood size:{}'.format(best_weak_asym[1], best_weak_asym_n[1]))\n",
    "print('best COV score:{:.3f}, neighborhood size:{}'.format(best_weak_asym[2], best_weak_asym_n[2]))\n",
    "print()\n",
    "print('Best neigborhood sizes for strong-generalization are:')\n",
    "print('best HR score:{:.3f}, neighborhood size:{}'.format(best_strong[0], best_strong_n[0]))\n",
    "print('best MRR score:{:.3f}, neighborhood size:{}'.format(best_strong[1], best_strong_n[1]))\n",
    "print('best COV score:{:.3f}, neighborhood size:{}'.format(best_strong[2], best_strong_n[2]))\n",
    "print()\n",
    "print('Best neigborhood sizes for strong-generalization asymmetric are:')\n",
    "print('best HR score:{:.3f}, neighborhood size:{}'.format(best_strong_asym[0], best_strong_asym_n[0]))\n",
    "print('best MRR score:{:.3f}, neighborhood size:{}'.format(best_strong_asym[1], best_strong_asym_n[1]))\n",
    "print('best COV score:{:.3f}, neighborhood size:{}'.format(best_strong_asym[2], best_strong_asym_n[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final analysis (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Provide an analysis on which model performs the best and explain why.\n",
    "2. Explain the difference in computational complexity of your models. Consider how the training and the recommendation generation differ for different models in terms of\n",
    "    - the amount of RAM,\n",
    "    - the amount of disk storage,\n",
    "    - the load on CPU.\n",
    "3. How else would you modify the model to improve either the quality of recommendations or computational performance? Describe at least one modification and its envisioned effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can see from the results above, that weak-generalization model performed better in every metric. This is because weak-generalized model have seen test users while training, while strong-generalized model not. So weak-generalized model got more data to learn, therefore it learned more. What about sym/asym models, in strong case they worked very similar, but in weak - asymmetric model outperformed symmetric in each metric. Maybe assymetricity helped to emphasize important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Weak:\n",
    "    - O(N) memory (to store two vectors to multiply)\n",
    "    - O(N^2) memory (matrix similarity)\n",
    "    - O(N^2M) time complexity for building matrix\n",
    "Assymetric\\symmetric cases same, because in first case we create weight matrix of size N by N.\n",
    "\n",
    "       Strong:\n",
    "    - O(N_train) memory\n",
    "    - O(max(N_test, N_items) * N_train) memory (matrix similarity and matrix of interactions)\n",
    "    - O(N_test * N_train * M) time complexity for building matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "64cd544b7330e8e73b8689d110cc075e8c836a404445c2b82c04f3ea96ea86ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
